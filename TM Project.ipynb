{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "from nltk.translate.chrf_score import sentence_chrf\n",
    "from sklearn import preprocessing \n",
    "import torch\n",
    "import warnings\n",
    "from scipy.stats import kendalltau, pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk.translate.gleu_score as gleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.nist_score import sentence_nist, corpus_nist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge import Rouge\n",
    "from nltk import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_names = ['cs_en', 'de_en', 'en_fi', 'en_zh', 'ru_en', 'zh_en', 'en_fi', 'en_zh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_en = pd.read_csv(\"corpus/cs-en/scores.csv\")\n",
    "de_en = pd.read_csv(\"corpus/de-en/scores.csv\")\n",
    "en_fi = pd.read_csv(\"corpus/en-fi/scores.csv\")\n",
    "en_zh = pd.read_csv(\"corpus/en-zh/scores.csv\")\n",
    "ru_en = pd.read_csv(\"corpus/ru-en/scores.csv\")\n",
    "zh_en = pd.read_csv(\"corpus/zh-en/scores.csv\")\n",
    "\n",
    "data_to_eng = [cs_en, de_en, ru_en, zh_en]\n",
    "data_from_eng = [en_fi, en_zh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_en = pd.DataFrame(data=cs_en,columns=['reference','translation','z-score'])\n",
    "de_en = pd.DataFrame(data=de_en,columns=['reference','translation','z-score'])\n",
    "en_fi = pd.DataFrame(data=en_fi,columns=['reference','translation','z-score'])\n",
    "en_zh = pd.DataFrame(data=en_zh,columns=['reference','translation','z-score'])\n",
    "ru_en = pd.DataFrame(data=ru_en,columns=['reference','translation','z-score'])\n",
    "zh_en = pd.DataFrame(data=zh_en,columns=['reference','translation','z-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # dataframes provided with the corresponding language translations\n",
    "\n",
    "    # assign dataset names\n",
    "    list_of_names = ['cs-en', 'de-en', 'ru-en', 'zh-en', 'en-fi', 'en-zh']\n",
    "\n",
    "    # create empty list\n",
    "    dataframes_list = []\n",
    "  \n",
    "    # append datasets into teh list\n",
    "    for i in range(len(list_of_names)):\n",
    "\n",
    "        temp_df = pd.read_csv(\"corpus/\"+ list_of_names[i]+\"/scores.csv\")\n",
    "\n",
    "        dataframes_list.append(temp_df)\n",
    "\n",
    "    return dataframes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty(df):\n",
    "    df = df.replace(r'^\\s*$', np.NaN, regex=True)\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_en = set(stopwords.words('english'))\n",
    "stop_fi = set(stopwords.words('finnish'))\n",
    "# stop_zh = chinese library needed \n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "def clean(text_list, lemmatize=False, stemmer=False, punctuation = True, stop_words=False, stop = stop_en):\n",
    "    \"\"\"\n",
    "    Function that a receives a list of strings and preprocesses it.\n",
    "    \n",
    "    :param text_list: List of strings.\n",
    "    :param lemmatize: Tag to apply lemmatization if True.\n",
    "    :param stemmer: Tag to apply the stemmer if True.\n",
    "    \"\"\"\n",
    "    updates = []\n",
    "    for j in tqdm(range(len(text_list))):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        \n",
    "        #LOWERCASE TEXT\n",
    "        text = text.lower()\n",
    "        \n",
    "        #REMOVE NUMERICAL DATA AND PUNCTUATION\n",
    "        if punctuation:\n",
    "            text = re.sub(\"[^a-zA-Z]\", ' ', text)\n",
    "        \n",
    "        #REMOVE TAGS (HTML)\n",
    "        text = BeautifulSoup(text).get_text()\n",
    "        \n",
    "        #REMOVE STOP WORDS - not needed \n",
    "        if stop_words:\n",
    "            text = \" \".join([word for word in text.split() if word not in stop])\n",
    "        \n",
    "        #LEMMATIZATION\n",
    "        if lemmatize:\n",
    "            text = \" \".join(lemma.lemmatize(word) for word in text.split())\n",
    "        \n",
    "        #STEMMER\n",
    "        if stemmer:\n",
    "            text = \" \".join(snowball_stemmer.stem(word) for word in text.split())\n",
    "        \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates\n",
    "\n",
    "def update_df(dataframe, list_updated, column):\n",
    "    dataframe.update(pd.DataFrame({column: list_updated}))\n",
    "    \n",
    "#updates = clean(df[\"translation\"], lemmatize = False, stemmer = False)\n",
    "#update_df(df, updates, \"translation\")\n",
    "\n",
    "def number_token(text):\n",
    "    \"\"\"\n",
    "    Function that receives a string of text and returns the string with \n",
    "    the cost formats within it substituted by the token #COST\n",
    "    \"\"\"\n",
    "    tokenized_text = re.sub('(\\d+|\\d+.\\d+)(| )','##',text)\n",
    "        \n",
    "    return tokenized_text\n",
    "\n",
    "def total_word_freq(text_list):\n",
    "    \"\"\"\n",
    "    Function that receives a list of strings and returns the frequency of each word\n",
    "    in the set of all strings.\n",
    "    \"\"\"\n",
    "    words_in_df = ' '.join(text_list).split()\n",
    "    # Count all words \n",
    "    freq = pd.Series(words_in_df).value_counts()\n",
    "    return freq\n",
    "\n",
    "# Fetch wordcount for each abstract\n",
    "def word_count(df):\n",
    "    word_count_ref  = df['reference'].apply(lambda x: len(str(x).split(\" \")))\n",
    "    word_count_tra  = df['translation'].apply(lambda x: len(str(x).split(\" \")))\n",
    "    df['word_count_ref'] = word_count_ref\n",
    "    df['word_count_tra'] = word_count_tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_token(df):\n",
    "\n",
    "    def transform_number(text):\n",
    "        \"\"\"\n",
    "        Function that receives a string of text and returns the string with \n",
    "        the cost formats within it substituted by the token #COST\n",
    "        \"\"\"\n",
    "        tokenized_text = re.sub('(\\d+|\\d+.\\d+)(| )','##',text)\n",
    "            \n",
    "        return tokenized_text\n",
    "\n",
    "    df[\"reference\"] = [transform_number(x) for x in df[\"reference\"]]\n",
    "    df[\"translation\"] = [transform_number(x) for x in df[\"translation\"]]\n",
    "\n",
    "def tokenize(df):\n",
    "    df['reference_token'] = [[x.split()] for x in df['reference']]\n",
    "    df['translation_token'] = [x.split() for x in df['translation']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_list = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_config = {\n",
    "        'lemmatize': False,\n",
    "        'stemmer': False,\n",
    "        'punctuation': True,\n",
    "        'stop_words': False,\n",
    "        'stop': stop_en\n",
    "        # lowercase\n",
    "        # remove punctuation\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer(translation, reference, print_matrix=False):\n",
    "    N = len(translation)\n",
    "    M = len(reference)\n",
    "    L = np.zeros((N, M))\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, M):\n",
    "            if min(i, j) == 0:\n",
    "                L[i, j] = max(i, j)\n",
    "            else:\n",
    "                deletion = L[i - 1, j] + 1\n",
    "                insertion = L[i, j - 1] + 1\n",
    "                sub = 1 if translation[i] != reference[j] else 0\n",
    "                substitution = L[i - 1, j - 1] + sub\n",
    "                L[i, j] = min(deletion, min(insertion, substitution))\n",
    "                # print(\"{} - {}: del {} ins {} sub {} s {}\".format(hyp[i], ref[j], deletion, insertion, substitution, sub))\n",
    "    if print_matrix:\n",
    "        print(\"WER matrix ({}x{}): \".format(N, M))\n",
    "        print(L)\n",
    "    return int(L[N - 1, M - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(df, name):\n",
    "    # get word count for each of reference and translation\n",
    "    word_count(df)\n",
    "\n",
    "    # apply baseline bleu model\n",
    "    baseline_bleu(df)\n",
    "\n",
    "    # apply sacre bleu\n",
    "    sacre_bleu(df)\n",
    "\n",
    "    # apply NIST model\n",
    "    nist(df)\n",
    "\n",
    "    # apply the rouge model\n",
    "    rouge_1(df)\n",
    "\n",
    "    # apply the bleu-rouge f1\n",
    "    bleu_rouge(df)\n",
    "\n",
    "    # apply meteor model\n",
    "    meteor(df)\n",
    "\n",
    "    # apply charF\n",
    "    charf(df)\n",
    "\n",
    "    # apply word embedding\n",
    "    #run_word_embedding(df, name)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate_models(df):  # TODO for laser\n",
    "    model_list = ['bleu', 'sacre_bleu', 'rouge', 'bleu_rouge', 'meteor', 'charf']\n",
    "    correl_df = pd.DataFrame()\n",
    "    # set indices\n",
    "    for model in model_list:\n",
    "        reg = RegressionReport()\n",
    "        correl_df[model] = reg.compute(df[model], df['z-score'])\n",
    "\n",
    "    return correl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionReport:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = [Pearson(), Kendall(), Spearman()]\n",
    "\n",
    "    def compute(self, x: np.array, y: np.array) -> float:\n",
    "        \"\"\"Computes Kendall correlation.\n",
    "        :param x: predicted scores.\n",
    "        :param x: ground truth scores.\n",
    "        :return: Kendall Tau correlation value.\n",
    "        \"\"\"\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return {metric.name: metric.compute(x, y) for metric in self.metrics}\n",
    "        \n",
    "class Kendall:\n",
    "    def __init__(self):\n",
    "        self.name = \"kendall\"\n",
    "\n",
    "    def compute(self, x: np.array, y: np.array) -> float:\n",
    "        \"\"\"Computes Kendall correlation.\n",
    "        :param x: predicted scores.\n",
    "        :param x: ground truth scores.\n",
    "        :return: Kendall Tau correlation value.\n",
    "        \"\"\"\n",
    "        return torch.tensor(kendalltau(x, y)[0], dtype=torch.float32)\n",
    "\n",
    "\n",
    "class Pearson:\n",
    "    def __init__(self):\n",
    "        self.name = \"pearson\"\n",
    "\n",
    "    def compute(self, x: np.array, y: np.array) -> torch.Tensor:\n",
    "        \"\"\"Computes Pearson correlation.\n",
    "        :param x: predicted scores.\n",
    "        :param x: ground truth scores.\n",
    "        :return: Pearson correlation value.\n",
    "        \"\"\"\n",
    "        return torch.tensor(pearsonr(x, y)[0], dtype=torch.float32)\n",
    "\n",
    "\n",
    "class Spearman:\n",
    "    def __init__(self):\n",
    "        self.name = \"spearman\"\n",
    "\n",
    "    def compute(self, x: np.array, y: np.array) -> float:\n",
    "        \"\"\"Computes Spearman correlation.\n",
    "        :param x: predicted scores.\n",
    "        :param x: ground truth scores.\n",
    "        Return:\n",
    "            - Spearman correlation value.\n",
    "        \"\"\"\n",
    "        return torch.tensor(spearmanr(x, y)[0], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df=[]\n",
    "correlations = []\n",
    "    \n",
    "list_of_names = ['cs-en', 'de-en', 'ru-en', 'zh-en', 'en-fi', 'en-zh']\n",
    "\n",
    "language_list_to_en = language_list[:-2]\n",
    "\n",
    "for name, df in enumerate(language_list_to_en):\n",
    "\n",
    "    df_size = df.shape[0]\n",
    "    print(\"Cleaning \" + list_of_names[name])\n",
    "\n",
    "    updates = clean(df[\"reference\"], lemmatize=preprocess_config['lemmatize'], stemmer=preprocess_config['stemmer'], stop_words=preprocess_config['stop_words'], stop=preprocess_config['stop'])\n",
    "    update_df(df, updates, \"reference\")\n",
    "\n",
    "    updates = clean(df[\"translation\"], lemmatize=preprocess_config['lemmatize'], stemmer=preprocess_config['stemmer'], stop_words=preprocess_config['stop_words'], stop=preprocess_config['stop'])\n",
    "    update_df(df, updates, \"translation\")\n",
    "\n",
    "    df = remove_empty(df)\n",
    "    print(df.shape[0]/df_size)\n",
    "\n",
    "    number_token(df)\n",
    "    df = tokenize(df)\n",
    "\n",
    "    print(\"Running models for \" + list_of_names[name])\n",
    "    final_df.append(run_models(df, list_of_names[name]))\n",
    "    \n",
    "    model_list = ['bleu', 'sacre_bleu', 'rouge', 'bleu_rouge', 'meteor', 'charf']\n",
    "\n",
    "    for i in model_list:\n",
    "        print(kendalltau(de_en['z-score'], de_en[i]))\n",
    "        print(pearsonr(de_en['z-score'], de_en[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updates = clean(cs_en[\"reference\"], lemmatize=preprocess_config['lemmatize'], stemmer=preprocess_config['stemmer'], stop_words=preprocess_config['stop_words'], stop=preprocess_config['stop'])\n",
    "update_df(cs_en, updates, \"reference\")\n",
    "\n",
    "updates = clean(cs_en[\"translation\"], lemmatize=preprocess_config['lemmatize'], stemmer=preprocess_config['stemmer'], stop_words=preprocess_config['stop_words'], stop=preprocess_config['stop'])\n",
    "update_df(cs_en, updates, \"translation\")\n",
    "\n",
    "cs_en = remove_empty(cs_en)\n",
    "\n",
    "number_token(cs_en)\n",
    "cs_en = tokenize(cs_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations.append(evaluate_models(de_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_en.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>z-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Her timeless pace measures them when they equi...</td>\n",
       "      <td>Their slow speed was measured by researchers o...</td>\n",
       "      <td>-0.345024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He said the areas offer quiet meeting points b...</td>\n",
       "      <td>He said the spaces provided calm meeting point...</td>\n",
       "      <td>0.903800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For businessmen at the B 27, it's only a small...</td>\n",
       "      <td>This is only a small consolation for businesse...</td>\n",
       "      <td>0.700503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This ability may be born or developed with gen...</td>\n",
       "      <td>This ability may be innate, or may develop as ...</td>\n",
       "      <td>-1.256572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Because they prefer water temperatures around ...</td>\n",
       "      <td>They generally only come to the surface in win...</td>\n",
       "      <td>0.293909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21699</th>\n",
       "      <td>Lt. Cmdr. Patrick Evans, a press officer at th...</td>\n",
       "      <td>Lt. Cmdr. Patrick Evans, a Pentagon spokesman,...</td>\n",
       "      <td>1.246459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21700</th>\n",
       "      <td>\"To give an example: If I ask him something th...</td>\n",
       "      <td>\"To give an example: If I ask him what happene...</td>\n",
       "      <td>0.792878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21701</th>\n",
       "      <td>One reason that not all neighbours view this a...</td>\n",
       "      <td>One reason for not all neighbours seeing this ...</td>\n",
       "      <td>0.597068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21702</th>\n",
       "      <td>Profit before interest and tax increased from ...</td>\n",
       "      <td>Profits before interest and taxes increased fr...</td>\n",
       "      <td>-0.305719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21703</th>\n",
       "      <td>Britain and the United States will clash on Sa...</td>\n",
       "      <td>Britain and the United States will meet on Sat...</td>\n",
       "      <td>0.995212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21704 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reference  \\\n",
       "0      Her timeless pace measures them when they equi...   \n",
       "1      He said the areas offer quiet meeting points b...   \n",
       "2      For businessmen at the B 27, it's only a small...   \n",
       "3      This ability may be born or developed with gen...   \n",
       "4      Because they prefer water temperatures around ...   \n",
       "...                                                  ...   \n",
       "21699  Lt. Cmdr. Patrick Evans, a press officer at th...   \n",
       "21700  \"To give an example: If I ask him something th...   \n",
       "21701  One reason that not all neighbours view this a...   \n",
       "21702  Profit before interest and tax increased from ...   \n",
       "21703  Britain and the United States will clash on Sa...   \n",
       "\n",
       "                                             translation   z-score  \n",
       "0      Their slow speed was measured by researchers o... -0.345024  \n",
       "1      He said the spaces provided calm meeting point...  0.903800  \n",
       "2      This is only a small consolation for businesse...  0.700503  \n",
       "3      This ability may be innate, or may develop as ... -1.256572  \n",
       "4      They generally only come to the surface in win...  0.293909  \n",
       "...                                                  ...       ...  \n",
       "21699  Lt. Cmdr. Patrick Evans, a Pentagon spokesman,...  1.246459  \n",
       "21700  \"To give an example: If I ask him what happene...  0.792878  \n",
       "21701  One reason for not all neighbours seeing this ...  0.597068  \n",
       "21702  Profits before interest and taxes increased fr... -0.305719  \n",
       "21703  Britain and the United States will meet on Sat...  0.995212  \n",
       "\n",
       "[21704 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = train['text'].values, train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_NN(df):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # embedding layer\n",
    "    model.add(Embedding(size_of_vocabulary, 300, input_length=100, trainable=True))\n",
    "\n",
    "    # lstm layer\n",
    "    model.add(LSTM(128, return_sequences=True, dropout=0.2))\n",
    "\n",
    "    # Global Maxpooling\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # Dense Layer\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Add loss function, metrics, optimizer\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"acc\"])\n",
    "\n",
    "    # Adding callbacks\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True, verbose=1)\n",
    "\n",
    "    # Print summary of model\n",
    "    #print(model.summary())\n",
    "\n",
    "    history = model.fit(np.array(x_tr_seq), np.array(y_tr), batch_size=128, epochs=10,\n",
    "                        validation_data=(np.array(x_val_seq), np.array(y_val)), verbose=1, callbacks=[es, mc])\n",
    "\n",
    "    model = load_model('best_model.h5')\n",
    "\n",
    "    # evaluation\n",
    "    _, val_acc = model.evaluate(x_val_seq, y_val, batch_size=128)\n",
    "    print(val_acc)\n",
    "\n",
    "def learn_embedd(df):\n",
    "    # Tokenize the sentences\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    # preparing vocabulary\n",
    "    tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "    # converting text into integer sequences\n",
    "    x_tr_seq = tokenizer.texts_to_sequences(x_tr)\n",
    "    x_val_seq = tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "    # padding to prepare sequences of same length\n",
    "    x_tr_seq = pad_sequences(x_tr_seq, maxlen=100)\n",
    "    x_val_seq = pad_sequences(x_val_seq, maxlen=100)\n",
    "\n",
    "\n",
    "def pretrain_embedding(df):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open('../input/glove6b/glove.6B.300d.txt')\n",
    "\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = np.zeros((size_of_vocabulary, 300))\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())\n",
    "\n",
    "#all words were made lowercase by default and that the punctuation was ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=0.9, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(cs_en[\"reference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_grams(corpus, top_k, n):\n",
    "    \"\"\"\n",
    "    Function that receives a list of documents (corpus) and extracts\n",
    "        the top k most frequent n-grams for that corpus.\n",
    "        \n",
    "    :param corpus: list of texts\n",
    "    :param top_k: int with the number of n-grams that we want to extract\n",
    "    :param n: n gram type to be considered \n",
    "             (if n=1 extracts unigrams, if n=2 extracts bigrams, ...)\n",
    "             \n",
    "    :return: Returns a sorted dataframe in which the first column \n",
    "        contains the extracted ngrams and the second column contains\n",
    "        the respective counts\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(ngram_range=(n, n), max_features=2000).fit(corpus)\n",
    "    \n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    \n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    \n",
    "    words_freq = []\n",
    "    for word, idx in vec.vocabulary_.items():\n",
    "        words_freq.append((word, sum_words[0, idx]))\n",
    "        \n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    top_df = pd.DataFrame(words_freq[:top_k])\n",
    "    top_df.columns = [\"Ngram\", \"Freq\"]\n",
    "    return top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_df = get_top_n_grams(cs_en[\"reference\"], top_k=20, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frequencies(top_df):\n",
    "    \"\"\"\n",
    "    Function that receives a dataframe from the \"get_top_n_grams\" function\n",
    "    and plots the frequencies in a bar plot.\n",
    "    \"\"\"\n",
    "    x_labels = top_df[\"Ngram\"][:30]\n",
    "    y_pos = np.arange(len(x_labels))\n",
    "    values = top_df[\"Freq\"][:30]\n",
    "    plt.bar(y_pos, values, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, x_labels)\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.title('Words')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frequencies(top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfTransformer()\n",
    "tfidf_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature names\n",
    "feature_names = cv.get_feature_names()\n",
    " \n",
    "# fetch document for which keywords needs to be extracted\n",
    "doc = cs_en[\"reference\"][53]\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector = tfidf_vectorizer.transform(cv.transform([doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_scores(feature_names, document_vector):\n",
    "    \"\"\"\n",
    "    Function that creates a dictionary with the TF-IDF score for each feature.\n",
    "    :param feature_names: list with all the feature words.\n",
    "    :param document_vector: vector containing the extracted features for a specific document\n",
    "    \n",
    "    :return: returns a sorted dictionary \"feature\":\"score\".\n",
    "    \"\"\"\n",
    "    feature2score = {}\n",
    "    for i in range(len(feature_names)):\n",
    "        feature2score[feature_names[i]] = document_vector[0][i]    \n",
    "    return sorted(feature2score.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_feature_scores(feature_names, tf_idf_vector.toarray())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "#print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "#print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "#print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
    "from rouge import Rouge\n",
    "from nltk.translate.nist_score import sentence_nist\n",
    "from nltk.translate.chrf_score import sentence_chrf\n",
    "from sklearn import preprocessing\n",
    "# check pytorch version\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measures precision\n",
    "def baseline_bleu(df):\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    df['bleu'] = df.apply(lambda x: sentence_bleu(x['reference_token'], x['translation_token'], weights=(1,0,0,0), smoothing_function=smoothie), axis=1)\n",
    "    return df \n",
    "\n",
    "def sacre_bleu(df):\n",
    "    df['sacre_bleu'] = df.apply(lambda x: sacrebleu.sentence_bleu(x['reference_toke'], x['translation_token']).score, axis=1)\n",
    "    df = df.dropna()\n",
    "    return df \n",
    "\n",
    "def nist(df):\n",
    "    # tokenization happens inside nist\n",
    "    df['nist'] = df.apply(lambda x: sentence_nist(x['reference'], x['translation']),axis=1)\n",
    "    return df \n",
    "\n",
    "# measures recall\n",
    "def rouge_1(df):\n",
    "    rouge = Rouge()\n",
    "    df['rouge'] = df.apply(lambda x: rouge.get_scores(x['translation'], x['reference'], avg=True)['rouge-1']['f'],axis=1) \n",
    "    return df\n",
    "\n",
    "def bleu_rouge(df):\n",
    "    df['bleu_rouge'] = 2 * (df['bleu'] * df['rouge']) / (df['bleu'] + df['rouge'])\n",
    "    df['bleu_rouge'] = df['bleu_rouge'].replace(np.nan, 0)\n",
    "    return df\n",
    "\n",
    "def meteor(df):\n",
    "    df['meteor'] = df.apply(lambda x: meteor_score([x['reference']], x['translation']),axis=1)\n",
    "    #If no words match during the method returns the score of 0\n",
    "    return df\n",
    "\n",
    "def charf(df):\n",
    "    df['charf'] = df.apply(lambda x: sentence_chrf([x['reference']], x['translation']),axis=1)\n",
    "    return df\n",
    "\n",
    "import sacrebleu\n",
    "def sacre_bleu(df):\n",
    "    df['sacre_bleu'] = df.apply(lambda x: sacrebleu.corpus_bleu(x['reference'], x['translation']).score, axis=1)\n",
    "    #x = df['sacre_bleu'].values.reshape(-1, 1) #returns a numpy array\n",
    "    #min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    #x_scaled = min_max_scaler.fit_transform(x)\n",
    "    #df['sacre_bleu'] = pd.DataFrame(x_scaled)\n",
    "    return df \n",
    "\n",
    "def charf(df):\n",
    "    df['charf'] = df.apply(lambda x: sentence_chrf([x['reference']], x['translation']),axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cs = cs_en.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_1(clean_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rouge'] = df.apply(lambda x: rouge.get_scores(x['translation'], x['reference'], avg=True)['rouge-1']['f'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates = clean(ru_en[\"reference\"], lemmatize=False, stemmer=False, punctuation = True, stop_words=False, stop=stop_en)\n",
    "update_df(ru_en, updates, \"reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_en = ru_en.replace(r'^\\s*$', np.NaN, regex=True)\n",
    "ru_en = ru_en.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_en[ru_en['reference'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_en = ru_en.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nist(ru_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ref_embedding = np.load('corpus/de-en/laser.reference_embeds.npy')\n",
    "translation_embedding = np.load('corpus/de-en/laser.translation_embeds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def plot_roc(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    \n",
    "def print_scores(clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('F1 score: {:3f}'.format(f1_score(y_test, y_pred)))\n",
    "    print('AUC score: {:3f}'.format(roc_auc_score(y_test, y_pred)))\n",
    "\n",
    "lr = LogisticRegression()\n",
    "print_scores(lr, X_train, y_train, X_test, y_test)\n",
    "plot_roc(lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed a word2vec with the ingredients\n",
    "w2v = gensim.models.Word2Vec(list(data.ingredients), size=350, window=10, min_count=2, iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "def run_word_embedding(df, name):\n",
    "\n",
    "    tokenized_corpus = []\n",
    "\n",
    "    tokenized_corpus = [x for i, y in df['reference_token'].apply(list).iteritems() for x in y]\n",
    "    tokenized_corpus = [word for sent in tokenized_corpus for word in sent]\n",
    "    tokenized_corpus = [word for sent in df['translation_token'] for word in sent]\n",
    "    #[tokenized_corpus.append(word) for doc in df['reference_token'] for word in doc]\n",
    "    #[tokenized_corpus.append(word) for word in df['translation_token']]\n",
    "    #vocabulary = {word for doc in tokenized_corpus for word in doc}\n",
    "\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(set(tokenized_corpus))}\n",
    "\n",
    "    # load word embeddings \n",
    "    W1 = np.load('corpus/'+ str(name) +'/laser.reference_embeds.npy')\n",
    "    W2 = np.load('corpus/'+ str(name) +'/laser.translation_embeds.npy')\n",
    "\n",
    "    #training_pairs = build_word_embedding_training(tokenized_corpus, word2idx)\n",
    "\n",
    "    #W1, W2, losses = Skip_Gram(training_pairs, word2idx, epochs=2)\n",
    "\n",
    "    W = torch.from_numpy(W1) + torch.from_numpy(W2)\n",
    "    W = (torch.t(W)/2).clone().detach()\n",
    "\n",
    "    df['wordEmbDistance'] = get_word_embedding_distance(W, word2idx, df['reference_token'], df['translation_token'])\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "def get_word_embedding_distance(W, word2idx, reference, translation):\n",
    "    distances = []\n",
    "    for sent_idx in range(len(reference)):\n",
    "        distances.append(apply_word_embedding_distance(W, word2idx, reference.iloc[sent_idx], translation.iloc[sent_idx]))\n",
    "\n",
    "    return distances\n",
    "\n",
    "def apply_word_embedding_distance(W, word2idx, sentence1, sentence2):\n",
    "    distance = 0\n",
    "    for word1 in sentence1[0]:\n",
    "        for word2 in sentence2:\n",
    "            distance += euclidean_distances([W[word2idx[word1]].numpy()], [W[word2idx[word2]].numpy()])\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>z-score</th>\n",
       "      <th>reference_token</th>\n",
       "      <th>translation_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Her timeless pace measures them when they equi...</td>\n",
       "      <td>Their slow speed was measured by researchers o...</td>\n",
       "      <td>-0.345024</td>\n",
       "      <td>[[Her, timeless, pace, measures, them, when, t...</td>\n",
       "      <td>[Their, slow, speed, was, measured, by, resear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He said the areas offer quiet meeting points b...</td>\n",
       "      <td>He said the spaces provided calm meeting point...</td>\n",
       "      <td>0.903800</td>\n",
       "      <td>[[He, said, the, areas, offer, quiet, meeting,...</td>\n",
       "      <td>[He, said, the, spaces, provided, calm, meetin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For businessmen at the B 27, it's only a small...</td>\n",
       "      <td>This is only a small consolation for businesse...</td>\n",
       "      <td>0.700503</td>\n",
       "      <td>[[For, businessmen, at, the, B, 27,, it's, onl...</td>\n",
       "      <td>[This, is, only, a, small, consolation, for, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This ability may be born or developed with gen...</td>\n",
       "      <td>This ability may be innate, or may develop as ...</td>\n",
       "      <td>-1.256572</td>\n",
       "      <td>[[This, ability, may, be, born, or, developed,...</td>\n",
       "      <td>[This, ability, may, be, innate,, or, may, dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Because they prefer water temperatures around ...</td>\n",
       "      <td>They generally only come to the surface in win...</td>\n",
       "      <td>0.293909</td>\n",
       "      <td>[[Because, they, prefer, water, temperatures, ...</td>\n",
       "      <td>[They, generally, only, come, to, the, surface...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21699</th>\n",
       "      <td>Lt. Cmdr. Patrick Evans, a press officer at th...</td>\n",
       "      <td>Lt. Cmdr. Patrick Evans, a Pentagon spokesman,...</td>\n",
       "      <td>1.246459</td>\n",
       "      <td>[[Lt., Cmdr., Patrick, Evans,, a, press, offic...</td>\n",
       "      <td>[Lt., Cmdr., Patrick, Evans,, a, Pentagon, spo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21700</th>\n",
       "      <td>\"To give an example: If I ask him something th...</td>\n",
       "      <td>\"To give an example: If I ask him what happene...</td>\n",
       "      <td>0.792878</td>\n",
       "      <td>[[\"To, give, an, example:, If, I, ask, him, so...</td>\n",
       "      <td>[\"To, give, an, example:, If, I, ask, him, wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21701</th>\n",
       "      <td>One reason that not all neighbours view this a...</td>\n",
       "      <td>One reason for not all neighbours seeing this ...</td>\n",
       "      <td>0.597068</td>\n",
       "      <td>[[One, reason, that, not, all, neighbours, vie...</td>\n",
       "      <td>[One, reason, for, not, all, neighbours, seein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21702</th>\n",
       "      <td>Profit before interest and tax increased from ...</td>\n",
       "      <td>Profits before interest and taxes increased fr...</td>\n",
       "      <td>-0.305719</td>\n",
       "      <td>[[Profit, before, interest, and, tax, increase...</td>\n",
       "      <td>[Profits, before, interest, and, taxes, increa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21703</th>\n",
       "      <td>Britain and the United States will clash on Sa...</td>\n",
       "      <td>Britain and the United States will meet on Sat...</td>\n",
       "      <td>0.995212</td>\n",
       "      <td>[[Britain, and, the, United, States, will, cla...</td>\n",
       "      <td>[Britain, and, the, United, States, will, meet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21704 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reference  \\\n",
       "0      Her timeless pace measures them when they equi...   \n",
       "1      He said the areas offer quiet meeting points b...   \n",
       "2      For businessmen at the B 27, it's only a small...   \n",
       "3      This ability may be born or developed with gen...   \n",
       "4      Because they prefer water temperatures around ...   \n",
       "...                                                  ...   \n",
       "21699  Lt. Cmdr. Patrick Evans, a press officer at th...   \n",
       "21700  \"To give an example: If I ask him something th...   \n",
       "21701  One reason that not all neighbours view this a...   \n",
       "21702  Profit before interest and tax increased from ...   \n",
       "21703  Britain and the United States will clash on Sa...   \n",
       "\n",
       "                                             translation   z-score  \\\n",
       "0      Their slow speed was measured by researchers o... -0.345024   \n",
       "1      He said the spaces provided calm meeting point...  0.903800   \n",
       "2      This is only a small consolation for businesse...  0.700503   \n",
       "3      This ability may be innate, or may develop as ... -1.256572   \n",
       "4      They generally only come to the surface in win...  0.293909   \n",
       "...                                                  ...       ...   \n",
       "21699  Lt. Cmdr. Patrick Evans, a Pentagon spokesman,...  1.246459   \n",
       "21700  \"To give an example: If I ask him what happene...  0.792878   \n",
       "21701  One reason for not all neighbours seeing this ...  0.597068   \n",
       "21702  Profits before interest and taxes increased fr... -0.305719   \n",
       "21703  Britain and the United States will meet on Sat...  0.995212   \n",
       "\n",
       "                                         reference_token  \\\n",
       "0      [[Her, timeless, pace, measures, them, when, t...   \n",
       "1      [[He, said, the, areas, offer, quiet, meeting,...   \n",
       "2      [[For, businessmen, at, the, B, 27,, it's, onl...   \n",
       "3      [[This, ability, may, be, born, or, developed,...   \n",
       "4      [[Because, they, prefer, water, temperatures, ...   \n",
       "...                                                  ...   \n",
       "21699  [[Lt., Cmdr., Patrick, Evans,, a, press, offic...   \n",
       "21700  [[\"To, give, an, example:, If, I, ask, him, so...   \n",
       "21701  [[One, reason, that, not, all, neighbours, vie...   \n",
       "21702  [[Profit, before, interest, and, tax, increase...   \n",
       "21703  [[Britain, and, the, United, States, will, cla...   \n",
       "\n",
       "                                       translation_token  \n",
       "0      [Their, slow, speed, was, measured, by, resear...  \n",
       "1      [He, said, the, spaces, provided, calm, meetin...  \n",
       "2      [This, is, only, a, small, consolation, for, b...  \n",
       "3      [This, ability, may, be, innate,, or, may, dev...  \n",
       "4      [They, generally, only, come, to, the, surface...  \n",
       "...                                                  ...  \n",
       "21699  [Lt., Cmdr., Patrick, Evans,, a, Pentagon, spo...  \n",
       "21700  [\"To, give, an, example:, If, I, ask, him, wha...  \n",
       "21701  [One, reason, for, not, all, neighbours, seein...  \n",
       "21702  [Profits, before, interest, and, taxes, increa...  \n",
       "21703  [Britain, and, the, United, States, will, meet...  \n",
       "\n",
       "[21704 rows x 5 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(de_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "def word_mover_distance(df):\n",
    "    # remove stop words?\n",
    "    model = api.load(translation_embedding)\n",
    "    df['wmd'] = df.apply(lambda x: model.wmdistance([x['reference']], x['translation']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = de_en['translation_token'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyemd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-5fe9cc876c32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'wmd'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mde_en\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwmdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reference'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'translation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6877\u001b[0m         )\n\u001b[1;32m-> 6878\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6880\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DataFrame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 result = libreduction.compute_reduction(\n\u001b[0m\u001b[0;32m    296\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m                 )\n",
      "\u001b[1;32mpandas\\_libs\\reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.compute_reduction\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-112-5fe9cc876c32>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'wmd'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mde_en\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwmdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reference'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'translation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mwmdistance\u001b[1;34m(self, document1, document2, norm)\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[1;31m# If pyemd C extension is available, import it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m         \u001b[1;31m# If pyemd is attempted to be used, but isn't installed, ImportError will be raised in wmdistance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 881\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[0mpyemd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0memd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m         \u001b[1;31m# Remove out-of-vocabulary words.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyemd'"
     ]
    }
   ],
   "source": [
    "df['wmd'] = de_en.apply(lambda x: model.wmdistance([x['reference']], x['translation']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x93 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-9b3e86ab4626>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpus/de-en/laser.translation_embeds.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m         \"\"\"\n\u001b[1;32m-> 1630\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1631\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   1900\u001b[0m             \u001b[0mfin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m             \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1903\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[1;34m(text, encoding, errors)\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x93 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format('corpus/de-en/laser.translation_embeds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, min_count=1,workers=3, window =3, sg = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-6812bfe80b7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtotal_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"glove.6B.300d.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersect_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"glove.6B.300d.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlockf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m         \"\"\"\n\u001b[1;32m-> 1630\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1631\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1891\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1892\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1893\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1894\u001b[0m             \u001b[1;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     fobj = _shortcut_open(\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.300d.txt'"
     ]
    }
   ],
   "source": [
    "model_2 = Word2Vec(min_count=1)\n",
    "model_2.build_vocab(sentences)\n",
    "total_examples = model_2.corpus_count\n",
    "model = KeyedVectors.load_word2vec_format(\"glove.6B.300d.txt\", binary=False)\n",
    "model_2.build_vocab([list(model.vocab.keys())], update=True)\n",
    "model_2.intersect_word2vec_format(\"glove.6B.300d.txt\", binary=False, lockf=1.0)\n",
    "model_2.train(sentences, total_examples=total_examples, epochs=model_2.iter)\n",
    "\n",
    "# fit a 2d PCA model to the vectors\n",
    "X = model_2[model_1.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model_1.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-fc26edf7bad8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_mover_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mde_en\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-70-e9aee769f24b>\u001b[0m in \u001b[0;36mword_mover_distance\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mword_mover_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# remove stop words?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslation_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'wmd'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwmdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reference'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'translation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\downloader.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \"\"\"\n\u001b[0;32m    489\u001b[0m     \u001b[0m_create_base_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m     \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Incorrect model/corpus name\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\downloader.py\u001b[0m in \u001b[0;36m_get_filename\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[0mcorpora\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minformation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'corpora'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minformation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'models'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minformation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'corpora'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"file_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "word_mover_distance(de_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def similarity(embeddings_1, embeddings_2):\n",
    "    embeddings_1 = tf.convert_to_tensor(embeddings_1, np.float32)\n",
    "    embeddings_2 = tf.convert_to_tensor(embeddings_2.transpose(0,1), np.float32)\n",
    "    return torch.matmul(\n",
    "        embeddings_1, embeddings_2\n",
    "    )\n",
    "\n",
    "def difference(embeddings_1, embeddings_2):\n",
    "    return embeddings_1 - embeddings_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tf.convert_to_tensor(ref_embedding, np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tf.convert_to_tensor(translation_embedding.transpose(0,1), np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similarity(ref_embedding, translation_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_embedding - translation_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.scatter(clean_csen['z-score'], clean_csen['bleu'])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "from scipy.stats import pearsonr\n",
    "kendalltau(clean_enzh['z-score'], clean_enzh['bleu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(clean_enzh['z-score'], clean_enzh['bleu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kendalltau(clean_csen['z-score'], clean_csen['meteor'])\n",
    "pearsonr(clean_csen['z-score'], clean_csen['meteor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement bleurt (from versions: none)\n",
      "ERROR: No matching distribution found for bleurt\n",
      "WARNING: You are using pip version 20.2.4; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\doris\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install bleurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bleurt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-51cbd29a8d4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbleurt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bleurt'"
     ]
    }
   ],
   "source": [
    "from bleurt import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
